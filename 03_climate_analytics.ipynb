{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Data Analytics\n",
    "\n",
    "This notebook performs advanced analytics on the processed climate data, including trend analysis, correlation studies, and predictive modeling to extract insights for Singapore's climate resilience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# In a real Databricks environment, we would use:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression as SparkLR\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ClimateDataAnalytics\").getOrCreate()\n",
    "\n",
    "# Define data directories - in Databricks these would typically be in DBFS\n",
    "PROCESSED_DIR = \"/dbfs/FileStore/climate_resilience/processed\"\n",
    "ANALYTICS_DIR = \"/dbfs/FileStore/climate_resilience/analytics\"\n",
    "os.makedirs(ANALYTICS_DIR, exist_ok=True)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(\"Climate Data Analytics environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Data\n",
    "\n",
    "This function loads the processed climate data from Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    \"\"\"\n",
    "    Loads the processed climate data from Delta Lake\n",
    "    \"\"\"\n",
    "    print(\"Loading processed climate data for analytics...\")\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    try:\n",
    "        # In a real Databricks environment, we would use:\n",
    "        df_long = spark.read.format(\"delta\").load(\"/dbfs/FileStore/climate_resilience/delta/climate_data_long\").toPandas()\n",
    "        df_wide = spark.read.format(\"delta\").load(\"/dbfs/FileStore/climate_resilience/delta/climate_data_wide\").toPandas()\n",
    "        \n",
    "        data_dict['long'] = df_long\n",
    "        data_dict['wide'] = df_wide\n",
    "        \n",
    "        print(\"Loaded data from Delta Lake\")\n",
    "        return data_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from Delta Lake: {e}\")\n",
    "        print(\"Falling back to CSV files...\")\n",
    "        \n",
    "        try:\n",
    "            # Try to load the CSV files\n",
    "            long_path = os.path.join(PROCESSED_DIR, \"climate_data_long.csv\")\n",
    "            wide_path = os.path.join(PROCESSED_DIR, \"climate_data_wide.csv\")\n",
    "            \n",
    "            if os.path.exists(long_path):\n",
    "                df_long = pd.read_csv(long_path)\n",
    "                data_dict['long'] = df_long\n",
    "                print(f\"Loaded long format data from {long_path}\")\n",
    "            \n",
    "            if os.path.exists(wide_path):\n",
    "                df_wide = pd.read_csv(wide_path)\n",
    "                data_dict['wide'] = df_wide\n",
    "                print(f\"Loaded wide format data from {wide_path}\")\n",
    "                \n",
    "            if data_dict:\n",
    "                return data_dict\n",
    "            else:\n",
    "                print(\"No processed data files found\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading processed data: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Trend Analysis\n",
    "\n",
    "This function analyzes trends in climate indicators over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_trend_analysis(data_dict):\n",
    "    \"\"\"\n",
    "    Analyzes trends in climate indicators over time\n",
    "    \"\"\"\n",
    "    print(\"Performing trend analysis on climate indicators...\")\n",
    "    \n",
    "    if data_dict is None or 'long' not in data_dict or data_dict['long'] is None:\n",
    "        print(\"No data for trend analysis\")\n",
    "        return None\n",
    "    \n",
    "    df_long = data_dict['long']\n",
    "    \n",
    "    # Ensure Year is in datetime format\n",
    "    if 'Year' in df_long.columns and df_long['Year'].dtype != 'datetime64[ns]':\n",
    "        df_long['Year'] = pd.to_datetime(df_long['Year'].astype(str), format='%Y')\n",
    "    \n",
    "    # Extract numeric year for regression\n",
    "    df_long['Year_Numeric'] = df_long['Year'].dt.year\n",
    "    \n",
    "    # Initialize results dataframe\n",
    "    trend_results = []\n",
    "    \n",
    "    # Analyze trends for each indicator\n",
    "    for indicator in df_long['Indicator'].unique():\n",
    "        indicator_data = df_long[df_long['Indicator'] == indicator].copy()\n",
    "        \n",
    "        # Skip if too few data points\n",
    "        if len(indicator_data) < 5:\n",
    "            continue\n",
    "        \n",
    "        # Prepare data for regression\n",
    "        X = indicator_data['Year_Numeric'].values.reshape(-1, 1)\n",
    "        y = indicator_data['Value'].values\n",
    "        \n",
    "        # Fit linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Get model metrics\n",
    "        y_pred = model.predict(X)\n",
    "        r_squared = r2_score(y, y_pred)\n",
    "        slope = model.coef_[0]\n",
    "        \n",
    "        # Calculate additional trend metrics\n",
    "        earliest_year = indicator_data['Year_Numeric'].min()\n",
    "        latest_year = indicator_data['Year_Numeric'].max()\n",
    "        earliest_value = indicator_data[indicator_data['Year_Numeric'] == earliest_year]['Value'].iloc[0]\n",
    "        latest_value = indicator_data[indicator_data['Year_Numeric'] == latest_year]['Value'].iloc[0]\n",
    "        \n",
    "        total_change = latest_value - earliest_value\n",
    "        percent_change = (total_change / earliest_value) * 100 if earliest_value != 0 else float('inf')\n",
    "        avg_annual_change = total_change / (latest_year - earliest_year) if latest_year > earliest_year else 0\n",
    "        \n",
    "        # Store results\n",
    "        trend_results.append({\n",
    "            'Indicator': indicator,\n",
    "            'Slope': slope,\n",
    "            'R_Squared': r_squared,\n",
    "            'Total_Change': total_change,\n",
    "            'Percent_Change': percent_change,\n",
    "            'Avg_Annual_Change': avg_annual_change,\n",
    "            'Start_Year': earliest_year,\n",
    "            'End_Year': latest_year,\n",
    "            'Start_Value': earliest_value,\n",
    "            'End_Value': latest_value\n",
    "        })\n",
    "        \n",
    "        # Create trend visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(indicator_data['Year_Numeric'], indicator_data['Value'], alpha=0.7)\n",
    "        plt.plot(indicator_data['Year_Numeric'], y_pred, color='red', linewidth=2)\n",
    "        \n",
    "        plt.title(f'Trend Analysis: {indicator} in Singapore (1960-2023)')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel(indicator)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Add trend information to plot\n",
    "        trend_direction = \"Increasing\" if slope > 0 else \"Decreasing\"\n",
    "        plt.figtext(0.15, 0.85, \n",
    "                   f\"Trend: {trend_direction}\\nAnnual Change: {avg_annual_change:.4f}\\nRÂ²: {r_squared:.4f}\", \n",
    "                   bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(ANALYTICS_DIR, f\"{indicator.replace(' ', '_')}_trend.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Create trend results dataframe\n",
    "    trend_df = pd.DataFrame(trend_results)\n",
    "    \n",
    "    # Save trend results\n",
    "    trend_path = os.path.join(ANALYTICS_DIR, \"trend_analysis_results.csv\")\n",
    "    trend_df.to_csv(trend_path, index=False)\n",
    "    print(f\"Saved trend analysis results to {trend_path}\")\n",
    "    \n",
    "    return trend_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Correlations\n",
    "\n",
    "This function analyzes correlations between different climate indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(data_dict):\n",
    "    \"\"\"\n",
    "    Analyzes correlations between different climate indicators\n",
    "    \"\"\"\n",
    "    print(\"Analyzing correlations between climate indicators...\")\n",
    "    \n",
    "    if data_dict is None or 'wide' not in data_dict or data_dict['wide'] is None:\n",
    "        print(\"No data for correlation analysis\")\n",
    "        return None\n",
    "    \n",
    "    df_wide = data_dict['wide']\n",
    "    \n",
    "    # Get numeric columns (indicators)\n",
    "    indicator_cols = df_wide.select_dtypes(include=['number']).columns\n",
    "    indicator_cols = [col for col in indicator_cols if col not in ['Year_Numeric', 'Decade']]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    if len(indicator_cols) > 1:\n",
    "        corr_matrix = df_wide[indicator_cols].corr()\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\", \n",
    "                   vmin=-1, vmax=1, square=True, linewidths=.5)\n",
    "        \n",
    "        plt.title('Correlation Matrix of Climate Indicators for Singapore')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(ANALYTICS_DIR, \"correlation_heatmap.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save correlation matrix\n",
    "        corr_path = os.path.join(ANALYTICS_DIR, \"indicator_correlations.csv\")\n",
    "        corr_matrix.to_csv(corr_path)\n",
    "        print(f\"Saved correlation matrix to {corr_path}\")\n",
    "        \n",
    "        # Find strongest correlations\n",
    "        strong_correlations = []\n",
    "        \n",
    "        for i in range(len(indicator_cols)):\n",
    "            for j in range(i+1, len(indicator_cols)):\n",
    "                indicator1 = indicator_cols[i]\n",
    "                indicator2 = indicator_cols[j]\n",
    "                correlation = corr_matrix.iloc[i, j]\n",
    "                \n",
    "                if abs(correlation) > 0.5:  # Only include strong correlations\n",
    "                    strong_correlations.append({\n",
    "                        'Indicator1': indicator1,\n",
    "                        'Indicator2': indicator2,\n",
    "                        'Correlation': correlation,\n",
    "                        'Strength': \"Strong Positive\" if correlation > 0.7 else \n",
    "                                   \"Moderate Positive\" if correlation > 0.3 else\n",
    "                                   \"Strong Negative\" if correlation < -0.7 else\n",
    "                                   \"Moderate Negative\"\n",
    "                    })\n",
    "        \n",
    "        # Create strong correlations dataframe\n",
    "        strong_corr_df = pd.DataFrame(strong_correlations)\n",
    "        \n",
    "        # Save strong correlations\n",
    "        strong_corr_path = os.path.join(ANALYTICS_DIR, \"strong_correlations.csv\")\n",
    "        strong_corr_df.to_csv(strong_corr_path, index=False)\n",
    "        print(f\"Saved strong correlations to {strong_corr_path}\")\n",
    "        \n",
    "        return corr_matrix\n",
    "    else:\n",
    "        print(\"Not enough indicators for correlation analysis\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Predictive Models\n",
    "\n",
    "This function builds predictive models for climate indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predictive_models(data_dict):\n",
    "    \"\"\"\n",
    "    Builds predictive models for climate indicators\n",
    "    \"\"\"\n",
    "    print(\"Building predictive models for climate indicators...\")\n",
    "    \n",
    "    if data_dict is None or 'long' not in data_dict or data_dict['long'] is None:\n",
    "        print(\"No data for predictive modeling\")\n",
    "        return None\n",
    "    \n",
    "    df_long = data_dict['long']\n",
    "    \n",
    "    # Ensure Year is in datetime format\n",
    "    if 'Year' in df_long.columns and df_long['Year'].dtype != 'datetime64[ns]':\n",
    "        df_long['Year'] = pd.to_datetime(df_long['Year'].astype(str), format='%Y')\n",
    "    \n",
    "    # Extract numeric year for modeling\n",
    "    df_long['Year_Numeric'] = df_long['Year'].dt.year\n",
    "    \n",
    "    # Initialize results\n",
    "    model_results = []\n",
    "    future_predictions = []\n",
    "    \n",
    "    # Build models for each indicator\n",
    "    for indicator in df_long['Indicator'].unique():\n",
    "        indicator_data = df_long[df_long['Indicator'] == indicator].copy()\n",
    "        \n",
    "        # Skip if too few data points\n",
    "        if len(indicator_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Building model for {indicator}...\")\n",
    "        \n",
    "        # Prepare data for modeling\n",
    "        X = indicator_data['Year_Numeric'].values.reshape(-1, 1)\n",
    "        y = indicator_data['Value'].values\n",
    "        \n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Store model results\n",
    "        model_results.append({\n",
    "            'Indicator': indicator,\n",
    "            'MSE': mse,\n",
    "            'R_Squared': r2,\n",
    "            'Coefficient': model.coef_[0],\n",
    "            'Intercept': model.intercept_\n",
    "        })\n",
    "        \n",
    "        # Generate future predictions\n",
    "        future_years = np.array(range(2024, 2051)).reshape(-1, 1)\n",
    "        future_values = model.predict(future_years)\n",
    "        \n",
    "        # Store future predictions\n",
    "        for i, year in enumerate(future_years.flatten()):\n",
    "            future_predictions.append({\n",
    "                'Indicator': indicator,\n",
    "                'Year': int(year),\n",
    "                'Predicted_Value': future_values[i]\n",
    "            })\n",
    "        \n",
    "        # Create prediction visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot historical data\n",
    "        plt.scatter(indicator_data['Year_Numeric'], indicator_data['Value'], \n",
    "                   alpha=0.7, label='Historical Data')\n",
    "        \n",
    "        # Plot model fit on all data\n",
    "        all_years = indicator_data['Year_Numeric'].values.reshape(-1, 1)\n",
    "        all_pred = model.predict(all_years)\n",
    "        plt.plot(all_years, all_pred, color='blue', linewidth=2, label='Model Fit')\n",
    "        \n",
    "        # Plot future predictions\n",
    "        plt.plot(future_years, future_values, color='red', linestyle='--', \n",
    "                linewidth=2, label='Future Predictions')\n",
    "        \n",
    "        # Add confidence interval (simplified)\n",
    "        plt.fill_between(future_years.flatten(), \n",
    "                        future_values - 1.96 * np.sqrt(mse), \n",
    "                        future_values + 1.96 * np.sqrt(mse), \n",
    "                        color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "        \n",
    "        plt.title(f'Predictive Model: {indicator} in Singapore (1960-2050)')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel(indicator)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add model metrics to plot\n",
    "        plt.figtext(0.15, 0.85, \n",
    "                   f\"Model RÂ²: {r2:.4f}\\nMSE: {mse:.4f}\", \n",
    "                   bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(ANALYTICS_DIR, f\"{indicator.replace(' ', '_')}_prediction.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Create model results dataframe\n",
    "    model_df = pd.DataFrame(model_results)\n",
    "    \n",
    "    # Save model results\n",
    "    model_path = os.path.join(ANALYTICS_DIR, \"predictive_model_results.csv\")\n",
    "    model_df.to_csv(model_path, index=False)\n",
    "    print(f\"Saved model results to {model_path}\")\n",
    "    \n",
    "    # Create future predictions dataframe\n",
    "    future_df = pd.DataFrame(future_predictions)\n",
    "    \n",
    "    # Save future predictions\n",
    "    future_path = os.path.join(ANALYTICS_DIR, \"future_predictions.csv\")\n",
    "    future_df.to_csv(future_path, index=False)\n",
    "    print(f\"Saved future predictions to {future_path}\")\n",
    "    \n",
    "    return {'model_results': model_df, 'future_predictions': future_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Climate Vulnerability Index\n",
    "\n",
    "This function calculates and analyzes the Climate Vulnerability Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vulnerability_index(data_dict):\n",
    "    \"\"\"\n",
    "    Calculates and analyzes the Climate Vulnerability Index\n",
    "    \"\"\"\n",
    "    print(\"Calculating Climate Vulnerability Index...\")\n",
    "    \n",
    "    if data_dict is None or 'wide' not in data_dict or data_dict['wide'] is None:\n",
    "        print(\"No data for vulnerability index calculation\")\n",
    "        return None\n",
    "    \n",
    "    df_wide = data_dict['wide']\n",
    "    \n",
    "    # Check if Climate Vulnerability Index already exists\n",
    "    if 'Climate_Vulnerability_Index' in df_wide.columns:\n",
    "        print(\"Climate Vulnerability Index already calculated\")\n",
    "        \n",
    "        # Ensure Year is in datetime format\n",
    "        if 'Year' in df_wide.columns and df_wide['Year'].dtype != 'datetime64[ns]':\n",
    "            df_wide['Year'] = pd.to_datetime(df_wide['Year'].astype(str), format='%Y')\n",
    "        \n",
    "        # Extract numeric year\n",
    "        df_wide['Year_Numeric'] = df_wide['Year'].dt.year\n",
    "        \n",
    "        # Create vulnerability index visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df_wide['Year_Numeric'], df_wide['Climate_Vulnerability_Index'], \n",
    "                marker='o', linestyle='-', linewidth=2, color='darkred')\n",
    "        \n",
    "        plt.title('Climate Vulnerability Index for Singapore (1960-2023)')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Vulnerability Index')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Add reference lines for risk levels\n",
    "        plt.axhline(y=75, color='red', linestyle='--', alpha=0.7, label='High Risk')\n",
    "        plt.axhline(y=50, color='orange', linestyle='--', alpha=0.7, label='Medium Risk')\n",
    "        plt.axhline(y=25, color='green', linestyle='--', alpha=0.7, label='Low Risk')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(ANALYTICS_DIR, \"vulnerability_index.png\")\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save vulnerability index data\n",
    "        vuln_df = df_wide[['Year', 'Year_Numeric', 'Climate_Vulnerability_Index']]\n",
    "        \n",
    "        # Add component columns if they exist\n",
    "        component_cols = [col for col in df_wide.columns if '_Norm' in col]\n",
    "        if component_cols:\n",
    "            vuln_df = pd.concat([vuln_df, df_wide[component_cols]], axis=1)\n",
    "        \n",
    "        vuln_path = os.path.join(ANALYTICS_DIR, \"climate_vulnerability_index.csv\")\n",
    "        vuln_df.to_csv(vuln_path, index=False)\n",
    "        print(f\"Saved vulnerability index data to {vuln_path}\")\n",
    "        \n",
    "        return vuln_df\n",
    "    else:\n",
    "        print(\"Climate Vulnerability Index not found in data\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "This function orchestrates the analytics process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the analytics process\n",
    "    \"\"\"\n",
    "    print(f\"Starting climate data analytics at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Load the processed data\n",
    "    data_dict = load_processed_data()\n",
    "    \n",
    "    if data_dict is not None:\n",
    "        # Perform trend analysis\n",
    "        trend_df = perform_trend_analysis(data_dict)\n",
    "        \n",
    "        # Analyze correlations\n",
    "        corr_matrix = analyze_correlations(data_dict)\n",
    "        \n",
    "        # Build predictive models\n",
    "        model_results = build_predictive_models(data_dict)\n",
    "        \n",
    "        # Calculate vulnerability index\n",
    "        vuln_df = calculate_vulnerability_index(data_dict)\n",
    "        \n",
    "        print(f\"Climate data analytics completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"Analytics results are now ready for visualization and interpretation\")\n",
    "        \n",
    "        return {\n",
    "            'trend_analysis': trend_df,\n",
    "            'correlations': corr_matrix,\n",
    "            'model_results': model_results,\n",
    "            'vulnerability_index': vuln_df\n",
    "        }\n",
    "    else:\n",
    "        print(\"Analytics failed: No data available\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analytics process\n",
    "analytics_results = main()\n",
    "\n",
    "# Display trend analysis results if available\n",
    "if analytics_results is not None and 'trend_analysis' in analytics_results and analytics_results['trend_analysis'] is not None:\n",
    "    analytics_results['trend_analysis'].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
