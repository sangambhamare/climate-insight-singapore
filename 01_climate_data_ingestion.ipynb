{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Data Ingestion\n",
    "\n",
    "This notebook handles the ingestion of climate change indicators for Singapore from the World Bank dataset and prepares it for processing in the Databricks environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# In a real Databricks environment, we would use:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ClimateDataIngestion\").getOrCreate()\n",
    "\n",
    "# Define data directory - in Databricks this would typically be DBFS\n",
    "DATA_DIR = \"/dbfs/FileStore/climate_resilience/datasets\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Climate Data Ingestion environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Climate Data\n",
    "\n",
    "This function downloads climate change indicators for Singapore from the World Bank dataset via HDX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_climate_data():\n",
    "    \"\"\"\n",
    "    Downloads climate change indicators for Singapore from HDX/World Bank\n",
    "    \"\"\"\n",
    "    print(\"Downloading climate change indicators for Singapore...\")\n",
    "    \n",
    "    # URL for the climate change indicators dataset\n",
    "    url = \"https://data.humdata.org/dataset/world-bank-climate-change-indicators-for-singapore/resource/a0c494f7-4c53-4920-9315-b5aa0e652e21/download/climate-change-indicators-for-singapore.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Download the CSV file\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Save the CSV file\n",
    "            csv_path = os.path.join(DATA_DIR, \"climate_change_indicators_singapore.csv\")\n",
    "            with open(csv_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Successfully downloaded data to {csv_path}\")\n",
    "            return csv_path\n",
    "        else:\n",
    "            print(f\"Failed to download data: HTTP {response.status_code}\")\n",
    "            # For demo purposes, we'll create a sample file\n",
    "            return create_sample_data()\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data: {e}\")\n",
    "        # For demo purposes, we'll create a sample file\n",
    "        return create_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Data\n",
    "\n",
    "This function creates sample climate data for demonstration purposes when the actual data cannot be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data():\n",
    "    \"\"\"\n",
    "    Creates sample climate data for demonstration purposes\n",
    "    \"\"\"\n",
    "    print(\"Creating sample climate data for demonstration...\")\n",
    "    \n",
    "    # Create a sample dataframe with climate indicators\n",
    "    data = {\n",
    "        'Year': list(range(1960, 2024)),\n",
    "        'Country': ['Singapore'] * 64,\n",
    "        'CountryCode': ['SGP'] * 64,\n",
    "        'Indicator': ['Average Temperature'] * 64,\n",
    "        'IndicatorCode': ['EN.ATM.TEMP'] * 64,\n",
    "        'Value': [26 + 0.01 * i + 0.2 * (i // 20) for i in range(64)]  # Simulated rising temperature\n",
    "    }\n",
    "    \n",
    "    # Create additional indicators\n",
    "    indicators = [\n",
    "        ('Rainfall', 'EN.CLC.PRCP', [2000 + 10 * (i % 5) - 5 * (i // 10) for i in range(64)]),\n",
    "        ('CO2 Emissions', 'EN.ATM.CO2E.PC', [3 + 0.1 * i for i in range(64)]),\n",
    "        ('Forest Area', 'AG.LND.FRST.ZS', [20 - 0.2 * i + 0.1 * (i // 30) for i in range(64)]),\n",
    "        ('Renewable Energy', 'EG.FEC.RNEW.ZS', [0.5 + 0.05 * i for i in range(64)])\n",
    "    ]\n",
    "    \n",
    "    # Add each indicator to the dataframe\n",
    "    for name, code, values in indicators:\n",
    "        indicator_data = {\n",
    "            'Year': list(range(1960, 2024)),\n",
    "            'Country': ['Singapore'] * 64,\n",
    "            'CountryCode': ['SGP'] * 64,\n",
    "            'Indicator': [name] * 64,\n",
    "            'IndicatorCode': [code] * 64,\n",
    "            'Value': values\n",
    "        }\n",
    "        data_temp = pd.DataFrame(indicator_data)\n",
    "        data = {k: data[k] + list(data_temp[k]) for k in data.keys()}\n",
    "    \n",
    "    # Create dataframe and save to CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = os.path.join(DATA_DIR, \"climate_change_indicators_singapore_sample.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Created sample data at {csv_path}\")\n",
    "    return csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Additional Indicators\n",
    "\n",
    "This function fetches additional climate indicators from the DataBank API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_additional_indicators():\n",
    "    \"\"\"\n",
    "    Fetches additional indicators from the DataBank API\n",
    "    In a real implementation, this would use the actual API\n",
    "    \"\"\"\n",
    "    print(\"Fetching additional indicators from DataBank API...\")\n",
    "    \n",
    "    # In a real Databricks implementation, we would use:\n",
    "    # from databricks.data_api import ApiClient\n",
    "    # client = ApiClient()\n",
    "    # data = client.call_api('DataBank/indicator_data', query={'indicator': 'EN.CLC.HEAT.XD', 'country': 'SGP'})\n",
    "    \n",
    "    # For demonstration, we'll create sample additional indicators\n",
    "    indicators = [\n",
    "        ('Sea Level Rise', 'EN.CLC.SLR', [0 + 0.5 * i for i in range(64)]),\n",
    "        ('Extreme Weather Events', 'EN.CLC.EXTR', [2 + 0.1 * i for i in range(64)]),\n",
    "        ('Urban Heat Island Effect', 'EN.CLC.HEAT.XD', [1 + 0.2 * i for i in range(64)])\n",
    "    ]\n",
    "    \n",
    "    data = {\n",
    "        'Year': [],\n",
    "        'Country': [],\n",
    "        'CountryCode': [],\n",
    "        'Indicator': [],\n",
    "        'IndicatorCode': [],\n",
    "        'Value': []\n",
    "    }\n",
    "    \n",
    "    for name, code, values in indicators:\n",
    "        data['Year'].extend(list(range(1960, 2024)))\n",
    "        data['Country'].extend(['Singapore'] * 64)\n",
    "        data['CountryCode'].extend(['SGP'] * 64)\n",
    "        data['Indicator'].extend([name] * 64)\n",
    "        data['IndicatorCode'].extend([code] * 64)\n",
    "        data['Value'].extend(values)\n",
    "    \n",
    "    # Create dataframe and save to CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = os.path.join(DATA_DIR, \"additional_climate_indicators_singapore.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Created additional indicators data at {csv_path}\")\n",
    "    return csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Delta Lake\n",
    "\n",
    "This function prepares the data for storage in Delta Lake format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_delta_lake(csv_path):\n",
    "    \"\"\"\n",
    "    Prepares the data for storage in Delta Lake\n",
    "    \"\"\"\n",
    "    print(f\"Preparing data from {csv_path} for Delta Lake...\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # In a real Databricks environment, we would use:\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark_df.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/climate_resilience/delta/climate_data\")\n",
    "    \n",
    "    # For demonstration outside of Databricks, we'll save as parquet\n",
    "    parquet_path = os.path.join(DATA_DIR, \"climate_data.parquet\")\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    print(f\"Data prepared and saved to Delta Lake at /dbfs/FileStore/climate_resilience/delta/climate_data\")\n",
    "    return \"/dbfs/FileStore/climate_resilience/delta/climate_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "This function orchestrates the data ingestion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data ingestion process\n",
    "    \"\"\"\n",
    "    print(f\"Starting data ingestion process at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Download primary climate data\n",
    "    climate_data_path = download_climate_data()\n",
    "    \n",
    "    # Fetch additional indicators\n",
    "    additional_data_path = fetch_additional_indicators()\n",
    "    \n",
    "    # Prepare data for Delta Lake\n",
    "    delta_path1 = prepare_for_delta_lake(climate_data_path)\n",
    "    delta_path2 = prepare_for_delta_lake(additional_data_path)\n",
    "    \n",
    "    print(f\"Data ingestion completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"Data is now ready for processing in the Databricks environment\")\n",
    "    \n",
    "    return {\n",
    "        \"climate_data_path\": climate_data_path,\n",
    "        \"additional_data_path\": additional_data_path,\n",
    "        \"delta_paths\": [delta_path1, delta_path2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data ingestion process\n",
    "result = main()\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
