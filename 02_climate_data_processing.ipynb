{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Data Processing\n",
    "\n",
    "This notebook processes the ingested climate data, performing cleaning, transformation, and feature engineering to prepare it for analytics and visualization in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# In a real Databricks environment, we would use:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ClimateDataProcessing\").getOrCreate()\n",
    "\n",
    "# Define data directories - in Databricks these would typically be in DBFS\n",
    "DATA_DIR = \"/dbfs/FileStore/climate_resilience/datasets\"\n",
    "PROCESSED_DIR = \"/dbfs/FileStore/climate_resilience/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Climate Data Processing environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "This function loads the ingested climate data from Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the ingested climate data from Delta Lake\n",
    "    \"\"\"\n",
    "    print(\"Loading climate data for processing...\")\n",
    "    \n",
    "    try:\n",
    "        # In a real Databricks environment, we would use:\n",
    "        df = spark.read.format(\"delta\").load(\"/dbfs/FileStore/climate_resilience/delta/climate_data\").toPandas()\n",
    "        print(\"Loaded data from Delta Lake\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from Delta Lake: {e}\")\n",
    "        print(\"Falling back to parquet/CSV files...\")\n",
    "        \n",
    "        try:\n",
    "            # Try to load the parquet file first\n",
    "            parquet_path = os.path.join(DATA_DIR, \"climate_data.parquet\")\n",
    "            if os.path.exists(parquet_path):\n",
    "                df = pd.read_parquet(parquet_path)\n",
    "                print(f\"Loaded data from {parquet_path}\")\n",
    "            else:\n",
    "                # Fall back to CSV files\n",
    "                climate_csv = os.path.join(DATA_DIR, \"climate_change_indicators_singapore.csv\")\n",
    "                additional_csv = os.path.join(DATA_DIR, \"additional_climate_indicators_singapore.csv\")\n",
    "                \n",
    "                if os.path.exists(climate_csv):\n",
    "                    df1 = pd.read_csv(climate_csv)\n",
    "                    print(f\"Loaded data from {climate_csv}\")\n",
    "                else:\n",
    "                    df1 = pd.read_csv(os.path.join(DATA_DIR, \"climate_change_indicators_singapore_sample.csv\"))\n",
    "                    print(\"Loaded sample climate data\")\n",
    "                    \n",
    "                if os.path.exists(additional_csv):\n",
    "                    df2 = pd.read_csv(additional_csv)\n",
    "                    print(f\"Loaded data from {additional_csv}\")\n",
    "                    df = pd.concat([df1, df2], ignore_index=True)\n",
    "                else:\n",
    "                    df = df1\n",
    "                    \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "\n",
    "This function cleans the data by handling missing values, duplicates, and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the data by handling missing values, duplicates, and outliers\n",
    "    \"\"\"\n",
    "    print(\"Cleaning climate data...\")\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to clean\")\n",
    "        return None\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"Missing values before cleaning: {df_clean.isna().sum().sum()}\")\n",
    "    \n",
    "    # For numeric columns, fill missing values with the median of that indicator\n",
    "    numeric_cols = df_clean.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_clean[col].isna().any():\n",
    "            if 'Indicator' in df_clean.columns:\n",
    "                # Fill by indicator group\n",
    "                for indicator in df_clean['Indicator'].unique():\n",
    "                    mask = df_clean['Indicator'] == indicator\n",
    "                    df_clean.loc[mask, col] = df_clean.loc[mask, col].fillna(\n",
    "                        df_clean.loc[mask, col].median())\n",
    "            else:\n",
    "                # Fill with overall median\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    \n",
    "    # For categorical columns, fill with the most frequent value\n",
    "    cat_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        if df_clean[col].isna().any():\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])\n",
    "    \n",
    "    print(f\"Missing values after cleaning: {df_clean.isna().sum().sum()}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"Removed {initial_rows - len(df_clean)} duplicate rows\")\n",
    "    \n",
    "    # Handle outliers using IQR method for numeric columns\n",
    "    for col in numeric_cols:\n",
    "        if col == 'Year':  # Skip year column\n",
    "            continue\n",
    "            \n",
    "        if 'Indicator' in df_clean.columns:\n",
    "            # Handle outliers by indicator group\n",
    "            for indicator in df_clean['Indicator'].unique():\n",
    "                mask = df_clean['Indicator'] == indicator\n",
    "                series = df_clean.loc[mask, col]\n",
    "                \n",
    "                # Calculate IQR\n",
    "                Q1 = series.quantile(0.25)\n",
    "                Q3 = series.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                # Define bounds\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Cap outliers instead of removing them\n",
    "                df_clean.loc[mask & (df_clean[col] < lower_bound), col] = lower_bound\n",
    "                df_clean.loc[mask & (df_clean[col] > upper_bound), col] = upper_bound\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data\n",
    "\n",
    "This function transforms the data by creating new features and restructuring for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"\"\"\n",
    "    Transforms the data by creating new features and restructuring for analysis\n",
    "    \"\"\"\n",
    "    print(\"Transforming climate data...\")\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to transform\")\n",
    "        return None\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_transform = df.copy()\n",
    "    \n",
    "    # Ensure Year is treated as a datetime for time-based analysis\n",
    "    if 'Year' in df_transform.columns and df_transform['Year'].dtype != 'datetime64[ns]':\n",
    "        # Convert to string first to handle potential numeric years\n",
    "        df_transform['Year'] = pd.to_datetime(df_transform['Year'].astype(str), format='%Y')\n",
    "    \n",
    "    # Create decade column for trend analysis\n",
    "    if 'Year' in df_transform.columns:\n",
    "        df_transform['Decade'] = (df_transform['Year'].dt.year // 10) * 10\n",
    "    \n",
    "    # Pivot the data for easier analysis if in long format\n",
    "    if 'Indicator' in df_transform.columns and 'Value' in df_transform.columns:\n",
    "        # Create a wide format dataframe with indicators as columns\n",
    "        df_wide = df_transform.pivot_table(\n",
    "            index=['Year', 'Country', 'CountryCode'],\n",
    "            columns='Indicator',\n",
    "            values='Value',\n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Clean up column names\n",
    "        df_wide.columns.name = None\n",
    "        \n",
    "        # Save both formats\n",
    "        df_long = df_transform\n",
    "        \n",
    "        # Add the decade column to wide format too\n",
    "        if 'Decade' in df_long.columns:\n",
    "            df_wide = df_wide.merge(\n",
    "                df_long[['Year', 'Decade']].drop_duplicates(),\n",
    "                on='Year',\n",
    "                how='left'\n",
    "            )\n",
    "        \n",
    "        return {'long': df_long, 'wide': df_wide}\n",
    "    \n",
    "    return {'long': df_transform}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer Features\n",
    "\n",
    "This function engineers new features to enhance analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(data_dict):\n",
    "    \"\"\"\n",
    "    Engineers new features to enhance analysis capabilities\n",
    "    \"\"\"\n",
    "    print(\"Engineering features for climate data...\")\n",
    "    \n",
    "    if data_dict is None or 'long' not in data_dict or data_dict['long'] is None:\n",
    "        print(\"No data for feature engineering\")\n",
    "        return None\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # Process long format data\n",
    "    df_long = data_dict['long'].copy()\n",
    "    \n",
    "    # Calculate year-over-year changes for each indicator\n",
    "    if 'Indicator' in df_long.columns and 'Value' in df_long.columns:\n",
    "        # Sort by indicator and year\n",
    "        df_long = df_long.sort_values(['Indicator', 'Year'])\n",
    "        \n",
    "        # Calculate year-over-year change and percent change\n",
    "        df_long['YoY_Change'] = df_long.groupby('Indicator')['Value'].diff()\n",
    "        df_long['YoY_Pct_Change'] = df_long.groupby('Indicator')['Value'].pct_change() * 100\n",
    "        \n",
    "        # Calculate 5-year moving average\n",
    "        df_long['MA_5Year'] = df_long.groupby('Indicator')['Value'].transform(\n",
    "            lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Calculate cumulative change from baseline (first year)\n",
    "        df_long['Cumulative_Change'] = df_long.groupby('Indicator')['Value'].transform(\n",
    "            lambda x: x - x.iloc[0] if len(x) > 0 else 0\n",
    "        )\n",
    "        \n",
    "        result_dict['long'] = df_long\n",
    "    \n",
    "    # Process wide format data if available\n",
    "    if 'wide' in data_dict and data_dict['wide'] is not None:\n",
    "        df_wide = data_dict['wide'].copy()\n",
    "        \n",
    "        # Get numeric columns (indicators)\n",
    "        indicator_cols = df_wide.select_dtypes(include=['number']).columns\n",
    "        indicator_cols = [col for col in indicator_cols if col not in ['Decade']]\n",
    "        \n",
    "        # Calculate correlation matrix between indicators\n",
    "        if len(indicator_cols) > 1:\n",
    "            corr_matrix = df_wide[indicator_cols].corr()\n",
    "            \n",
    "            # Save correlation matrix\n",
    "            corr_path = os.path.join(PROCESSED_DIR, \"indicator_correlations.csv\")\n",
    "            corr_matrix.to_csv(corr_path)\n",
    "            print(f\"Saved correlation matrix to {corr_path}\")\n",
    "        \n",
    "        # Create composite indicators if relevant indicators exist\n",
    "        \n",
    "        # Example: Climate Vulnerability Index\n",
    "        temp_col = next((col for col in indicator_cols if 'Temperature' in col), None)\n",
    "        rainfall_col = next((col for col in indicator_cols if 'Rainfall' in col), None)\n",
    "        co2_col = next((col for col in indicator_cols if 'CO2' in col), None)\n",
    "        \n",
    "        if temp_col and rainfall_col:\n",
    "            # Normalize the indicators\n",
    "            df_wide['Temp_Norm'] = (df_wide[temp_col] - df_wide[temp_col].min()) / (df_wide[temp_col].max() - df_wide[temp_col].min())\n",
    "            df_wide['Rainfall_Norm'] = (df_wide[rainfall_col] - df_wide[rainfall_col].min()) / (df_wide[rainfall_col].max() - df_wide[rainfall_col].min())\n",
    "            \n",
    "            # Create climate vulnerability index (example formula)\n",
    "            df_wide['Climate_Vulnerability_Index'] = 0.6 * df_wide['Temp_Norm'] + 0.4 * (1 - df_wide['Rainfall_Norm'])\n",
    "            \n",
    "            if co2_col:\n",
    "                df_wide['CO2_Norm'] = (df_wide[co2_col] - df_wide[co2_col].min()) / (df_wide[co2_col].max() - df_wide[co2_col].min())\n",
    "                df_wide['Climate_Vulnerability_Index'] = 0.5 * df_wide['Climate_Vulnerability_Index'] + 0.5 * df_wide['CO2_Norm']\n",
    "        \n",
    "        result_dict['wide'] = df_wide\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "This function saves the processed data to Delta Lake format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(data_dict):\n",
    "    \"\"\"\n",
    "    Saves the processed data to Delta Lake format\n",
    "    \"\"\"\n",
    "    print(\"Saving processed climate data...\")\n",
    "    \n",
    "    if data_dict is None:\n",
    "        print(\"No data to save\")\n",
    "        return\n",
    "    \n",
    "    # Save long format\n",
    "    if 'long' in data_dict and data_dict['long'] is not None:\n",
    "        # In a real Databricks environment, we would use:\n",
    "        spark_df_long = spark.createDataFrame(data_dict['long'])\n",
    "        spark_df_long.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/climate_resilience/delta/climate_data_long\")\n",
    "        print(\"Saved long format data to Delta Lake at /dbfs/FileStore/climate_resilience/delta/climate_data_long\")\n",
    "        \n",
    "        # Also save as CSV for easier inspection\n",
    "        csv_long_path = os.path.join(PROCESSED_DIR, \"climate_data_long.csv\")\n",
    "        data_dict['long'].to_csv(csv_long_path, index=False)\n",
    "        print(f\"Saved long format data to {csv_long_path}\")\n",
    "    \n",
    "    # Save wide format\n",
    "    if 'wide' in data_dict and data_dict['wide'] is not None:\n",
    "        # In a real Databricks environment, we would use:\n",
    "        spark_df_wide = spark.createDataFrame(data_dict['wide'])\n",
    "        spark_df_wide.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/climate_resilience/delta/climate_data_wide\")\n",
    "        print(\"Saved wide format data to Delta Lake at /dbfs/FileStore/climate_resilience/delta/climate_data_wide\")\n",
    "        \n",
    "        # Also save as CSV for easier inspection\n",
    "        csv_wide_path = os.path.join(PROCESSED_DIR, \"climate_data_wide.csv\")\n",
    "        data_dict['wide'].to_csv(csv_wide_path, index=False)\n",
    "        print(f\"Saved wide format data to {csv_wide_path}\")\n",
    "    \n",
    "    print(\"All processed data saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "This function orchestrates the data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data processing pipeline\n",
    "    \"\"\"\n",
    "    print(f\"Starting data processing at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = load_data()\n",
    "    \n",
    "    if df is not None and not df.empty:\n",
    "        # Clean the data\n",
    "        df_clean = clean_data(df)\n",
    "        \n",
    "        # Transform the data\n",
    "        data_dict = transform_data(df_clean)\n",
    "        \n",
    "        # Engineer features\n",
    "        processed_data = engineer_features(data_dict)\n",
    "        \n",
    "        # Save the processed data\n",
    "        save_processed_data(processed_data)\n",
    "        \n",
    "        print(f\"Data processing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"Data is now ready for analytics and visualization\")\n",
    "        \n",
    "        return processed_data\n",
    "    else:\n",
    "        print(\"Data processing failed: No data available\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data processing pipeline\n",
    "processed_data = main()\n",
    "\n",
    "# Display a sample of the processed data\n",
    "if processed_data is not None and 'long' in processed_data:\n",
    "    processed_data['long'].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
